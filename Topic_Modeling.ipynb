{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "from tomotopy import LDAModel, TermWeight, ParallelScheme\n",
    "from typing import Dict, List, Optional, Any, Union, Iterable, Callable, Tuple\n",
    "from kneed import KneeLocator\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Union, List, Optional, Any\n",
    "\n",
    "\n",
    "class IAlgorithm(ABC):\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def algorithm_description(self) -> str:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(\n",
    "        self, documents: List[Dict[str, Any]]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class LatentDirichletAllocation(IAlgorithm):\n",
    "    algorithm_description = \"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Model Parameters\n",
    "        tw: Union[int, TermWeight] = TermWeight.ONE,\n",
    "        min_cf: int = 0,\n",
    "        min_df: int = 0,\n",
    "        rm_top: int = 0,\n",
    "        k: int = 6,\n",
    "        alpha: Union[float, Iterable[float]] = 0.1,\n",
    "        eta: float = 0.01,\n",
    "        seed: int = 42,\n",
    "        transform: Optional[Callable] = None,\n",
    "        # Training Parameters\n",
    "        iter: int = 5000,\n",
    "        workers: int = 10,\n",
    "        parallel=ParallelScheme.DEFAULT,\n",
    "        freeze_topics: bool = False,\n",
    "        callback_interval: int = 10,\n",
    "        callback: Optional[Callable] = None,\n",
    "        show_progress: bool = False,\n",
    "        **kwargs: Any \n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        LDA Algorithm, the base topic modeling algorithm on which most other\n",
    "        tm algorithms are based.\n",
    "\n",
    "        tomotopy LDA Model initialization parameters:\n",
    "\n",
    "        tw : Union[int, TermWeight]\n",
    "            term weighting scheme in TermWeight. The default value is\n",
    "            TermWeight.ONE\n",
    "        min_cf : int\n",
    "            minimum collection frequency of words. Words with a smaller collection frequency than min_cf are excluded from the model. The default value is 0, which means no words are excluded.\n",
    "        min_df : int\n",
    "            minimum document frequency of words. Words with a smaller document frequency than min_df are excluded from the model. The default value is 0, which means no words are excluded\n",
    "        rm_top : int\n",
    "            the number of top words to be removed. If you want to remove too common words from model, you can set this value to 1 or more. The default value is 0, which means no top words are removed.\n",
    "        k : Optional(int)\n",
    "            the number of topics between 1 ~ 32767\n",
    "            if k == None topic models from 0 to 100 will be calculated and the\n",
    "            optimal number of topics will be found by searching the knee of the\n",
    "            perplexity number of topics curve.\n",
    "        alpha : Union[float, Iterable[float]]\n",
    "            hyperparameter of Dirichlet distribution for document-topic, given as a single float in case of symmetric prior and as a list with length k of float in case of asymmetric prior.\n",
    "        eta : float\n",
    "            hyperparameter of Dirichlet distribution for topic-word\n",
    "        seed : int\n",
    "            random seed. The default value is a random number from std::random_device{} in C++\n",
    "        transform : Callable[dict, dict]\n",
    "            a callable object to manipulate arbitrary keyword arguments for a specific topic model\n",
    "\n",
    "        Training Parameters\n",
    "        iter : int\n",
    "            the number of iterations of Gibbs-sampling\n",
    "        workers : int\n",
    "            an integer indicating the number of workers to perform samplings. If workers is 0, the number of cores in the system will be used.\n",
    "        parallel : Union[int, ParallelScheme]\n",
    "            the parallelism scheme for training. the default value is ParallelScheme.DEFAULT which means that tomotopy selects the best scheme by model.\n",
    "        freeze_topics : bool\n",
    "            prevents to create a new topic when training. Only valid for HLDAModel\n",
    "        callback_interval : int\n",
    "            the intderval of calling callback function. If callback_interval <= 0, callback function is called at the beginning and the end of training.\n",
    "        callback : Callable[[LDAModel, int, int], None]\n",
    "            a callable object which is called every callback_interval iterations. It receives three arguments: the current model, the current number of iterations, and the total number of iterations.\n",
    "        show_progress : bool\n",
    "            If True, it shows progress bar during training using tqdm package.\n",
    "\n",
    "        \"\"\"\n",
    "        # Setting the model Parameters\n",
    "        self.tw = tw\n",
    "        self.min_cf = min_cf\n",
    "        self.min_df = min_df\n",
    "        self.rm_top = rm_top\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.eta = eta\n",
    "        self.seed = seed\n",
    "        self.transform = transform\n",
    "\n",
    "        # Setting the training parameters\n",
    "        self.iter = iter\n",
    "        self.workers = workers\n",
    "        self.parallel = parallel\n",
    "        self.freeze_topics = freeze_topics\n",
    "        self.callback_interval = callback_interval\n",
    "        self.callback = callback\n",
    "        self.show_progress = show_progress\n",
    "\n",
    "    def _visualize(self, results: Dict[str, Union[Dict, str, List]]) -> Dict[str, Union[Dict, str, List]]:\n",
    "        \"\"\"\n",
    "        Creates Visualizations of the generated results including:\n",
    "\n",
    "        - Word Clouds for each topic.\n",
    "        - Histogram showing word counts per topic with labels.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        from wordcloud import WordCloud\n",
    "        import os\n",
    "\n",
    "        # Ensure the 'visualizations' directory exists\n",
    "        if not os.path.exists(\"visualizations\"):\n",
    "            os.makedirs(\"visualizations\")\n",
    "\n",
    "        # Plotting Word Clouds for each topic\n",
    "        topic_words = results[\"Topic Words\"]\n",
    "        num_topics = len(topic_words)\n",
    "        \n",
    "        # Determine the grid size for plotting all word clouds in one figure\n",
    "        cols = 3  # Number of columns in the grid\n",
    "        rows = (num_topics + cols - 1) // cols  # Calculate number of rows\n",
    "\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(6, 6))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, (topic_key, words) in enumerate(topic_words.items()):\n",
    "            # Convert list of tuples into a dictionary for WordCloud\n",
    "            word_freq = {word: prob for word, prob in words}\n",
    "            # Generate word cloud\n",
    "            wordcloud = WordCloud(width=400,\n",
    "                                  height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "            # Plot word cloud\n",
    "            ax = axes[i]\n",
    "            ax.imshow(wordcloud, interpolation='bilinear')\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f\"Topic {i}\", fontsize=14)\n",
    "\n",
    "        # Hide any unused subplots\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        wordclouds_filepath = os.path.join(\"visualizations\", \"TopicWordClouds.png\")\n",
    "        plt.savefig(wordclouds_filepath)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Adding the word clouds to the results\n",
    "        results[\"Topic Word Clouds Explanation\"] = \"\"\"\n",
    "        A plot that shows the word clouds for each topic. Each word cloud represents the top words in the topic, with the size of each word corresponding to its probability in the topic.\n",
    "        \"\"\"\n",
    "        results[\"Topic Word Clouds\"] = wordclouds_filepath\n",
    "\n",
    "        # Plotting Histogram of Word Counts per Topic\n",
    "        counts_per_topic = results[\"Counts Per Topic\"]  # Assuming this is a list of word counts per topic\n",
    "        topics = list(range(len(counts_per_topic)))  # List of topic indices\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        bars = ax.bar(topics, counts_per_topic)\n",
    "        ax.set_title(\"Word Counts per Topic\")\n",
    "        ax.set_xlabel(\"Topic\")\n",
    "        ax.set_ylabel(\"Word Count\")\n",
    "        ax.set_xticks(topics)\n",
    "        ax.set_xticklabels([f\"Topic {i}\" for i in topics],\n",
    "                           rotation=\"vertical\")\n",
    "\n",
    "        # Adding labels on top of each bar\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                yval + max(counts_per_topic) * 0.01,  # Adjust position above the bar\n",
    "                f'{int(yval)}',\n",
    "                ha='center',\n",
    "                va='bottom'\n",
    "            )\n",
    "\n",
    "        histogram_filepath = os.path.join(\"visualizations\", \"WordCountsPerTopic.png\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(histogram_filepath)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Adding the histogram to the results\n",
    "        results[\"Word Counts per Topic Explanation\"] = \"\"\"\n",
    "        A histogram that shows the total word counts for each topic.\n",
    "        This provides an overview of how many words are associated with each topic.\n",
    "        Each bar is labeled with the exact word count to make it easier to read.\n",
    "        \"\"\"\n",
    "        results[\"Word Counts per Topic\"] = histogram_filepath\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _find_k(self, documents: List[Dict[str, Any]]) -> Tuple[int, LDAModel]:\n",
    "        \"\"\"\n",
    "        Trains models with 2-5 topics and identifies the best fit via knee\n",
    "        of the perplexity curve.\n",
    "\n",
    "        !!! Does not work yet!!!\n",
    "        \"\"\"\n",
    "\n",
    "        test_k = [x for x in range(2, 15)]\n",
    "        perplexities = []\n",
    "        models = []\n",
    "        for k in test_k:\n",
    "            model = LDAModel(\n",
    "                tw=self.tw,\n",
    "                min_cf=self.min_cf,\n",
    "                min_df=self.min_df,\n",
    "                rm_top=self.rm_top,\n",
    "                k=k,\n",
    "                alpha=self.alpha,\n",
    "                eta=self.eta,\n",
    "                seed=self.seed,\n",
    "                transform=self.transform,\n",
    "            )\n",
    "            for document in documents:\n",
    "                words = document[\"AbstractNormalized\"]\n",
    "                model.add_doc(words)\n",
    "\n",
    "            model.train(\n",
    "                iter=self.iter,\n",
    "                workers=self.workers,\n",
    "                parallel=self.parallel,\n",
    "                freeze_topics=self.freeze_topics,\n",
    "                show_progress=self.show_progress,\n",
    "            )\n",
    "\n",
    "            print(k, \"Perplexity\", model.perplexity)\n",
    "            perplexities.append(model.perplexity)\n",
    "            models.append(model)\n",
    "\n",
    "        best_k = KneeLocator(\n",
    "            test_k, perplexities, curve=\"convex\", direction=\"decreasing\"\n",
    "        ).knee\n",
    "\n",
    "        # Plotting the Perplexity\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax.set_title(\"Perplexity Per Number of Topics\")\n",
    "        ax.set_xticks([x for x in test_k[::10]])\n",
    "        ax.set_xticklabels([x for x in test_k[::10]])\n",
    "        ax.set_xlabel(\"Number of Topics\")\n",
    "        ax.set_ylabel(\"Perplexity\")\n",
    "\n",
    "        ax.plot(test_k, perplexities)\n",
    "\n",
    "        # Adding a Marker for the Knee\n",
    "        ax.plot(\n",
    "            test_k[best_k : best_k + 1],\n",
    "            perplexities[best_k : best_k + 1],\n",
    "            marker=\"o\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "        fig.savefig(\n",
    "            os.path.join(\"visualizations\", \"PerplexityPerNumberOfTopics.png\")\n",
    "        )\n",
    "\n",
    "        return best_k, models[best_k]\n",
    "\n",
    "    def __call__(\n",
    "        self, documents: List[Dict[str, Any]], top_n=10\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "\n",
    "        # Making a deep copy of documents, so that the original documents will\n",
    "        # not be changed in this algorithm.\n",
    "        print(\"In LDA\")\n",
    "        documents = copy.deepcopy(documents)\n",
    "        # remove all documents that have no key Abstarct Normalized\n",
    "        len_documents_initially = len(documents)\n",
    "        documents = [\n",
    "            document\n",
    "            for document in documents\n",
    "            if \"AbstractNormalized\" in document.keys()\n",
    "        ]\n",
    "        print(\"Normalized Documents\")\n",
    "        # Performing the Analysis\n",
    "        if self.k is None:\n",
    "            # Searching for the optimal value of k if k=None\n",
    "            k, model = self._find_k(documents)\n",
    "        else:\n",
    "            # Performing the normal LDA if k is not None\n",
    "            print(\"Creating the model\")\n",
    "            model = LDAModel(\n",
    "                tw=self.tw,\n",
    "                min_cf=self.min_cf,\n",
    "                min_df=self.min_df,\n",
    "                rm_top=self.rm_top,\n",
    "                k=self.k,\n",
    "                alpha=self.alpha,\n",
    "                eta=self.eta,\n",
    "                seed=self.seed,\n",
    "                transform=self.transform,\n",
    "            )\n",
    "            print(\"Adding the Documents\")\n",
    "            for document in documents:\n",
    "                words = document[\"AbstractNormalized\"]\n",
    "                model.add_doc(words)\n",
    "\n",
    "            print(\"Training the model\")\n",
    "            model.train(\n",
    "                iter=self.iter,\n",
    "                workers=self.workers,\n",
    "                parallel=self.parallel,\n",
    "                freeze_topics=self.freeze_topics,\n",
    "                show_progress=self.show_progress,\n",
    "            )\n",
    "        print(\"Extracting the results\")\n",
    "     \n",
    "        # Extracting the results\n",
    "        results = {\n",
    "            \"Documents Analyzed\": len(documents),\n",
    "            \"Documents discarded because publication date could not be parsed\": len(\n",
    "                documents\n",
    "            )\n",
    "            - len_documents_initially,\n",
    "            \"Topic Words Explanation\": \"\"\"\n",
    "Topic Word distribution for every topic for the top 10 words.\n",
    "It is presented in form of a dictionary with items:\n",
    "    topic<k>_timestamp<t>: [(<word>, probability)...]\n",
    "            \"\"\",\n",
    "            \"Topic Words\": {},\n",
    "            \"Counts Per Topic Explanation\": \"\"\"\n",
    "The number of words allocated to each topic in the form of [n_words_topic_1, \n",
    "n_words_topic_2....])\n",
    "            \"\"\",\n",
    "            \"Counts Per Topic\": model.get_count_by_topics(),\n",
    "            \"Hyperparameters Explanation\": \"\"\"LDA Algorithm, the base topic modeling algorithm on which most other\n",
    "        tm algorithms are based.\n",
    "\n",
    "        tomotopy LDA Model initialization parameters:\n",
    "\n",
    "        tw : Union[int, TermWeight]\n",
    "            term weighting scheme in TermWeight. The default value is\n",
    "            TermWeight.ONE\n",
    "        min_cf : int\n",
    "            minimum collection frequency of words. Words with a smaller collection frequency than min_cf are excluded from the model. The default value is 0, which means no words are excluded.\n",
    "        min_df : int\n",
    "            minimum document frequency of words. Words with a smaller document frequency than min_df are excluded from the model. The default value is 0, which means no words are excluded\n",
    "        rm_top : int\n",
    "            the number of top words to be removed. If you want to remove too common words from model, you can set this value to 1 or more. The default value is 0, which means no top words are removed.\n",
    "        k : Optional(int)\n",
    "            the number of topics between 1 ~ 32767\n",
    "            if k == None topic models from 0 to 100 will be calculated and the\n",
    "            optimal number of topics will be found by searching the knee of the\n",
    "            perplexity number of topics curve.\n",
    "        alpha : Union[float, Iterable[float]]\n",
    "            hyperparameter of Dirichlet distribution for document-topic, given as a single float in case of symmetric prior and as a list with length k of float in case of asymmetric prior.\n",
    "        eta : float\n",
    "            hyperparameter of Dirichlet distribution for topic-word\n",
    "        seed : int\n",
    "            random seed. The default value is a random number from std::random_device{} in C++\n",
    "        transform : Callable[dict, dict]\n",
    "            a callable object to manipulate arbitrary keyword arguments for a specific topic model\n",
    "\n",
    "        Training Parameters\n",
    "        iter : int\n",
    "            the number of iterations of Gibbs-sampling\n",
    "        workers : int\n",
    "            an integer indicating the number of workers to perform samplings. If workers is 0, the number of cores in the system will be used.\n",
    "        parallel : Union[int, ParallelScheme]\n",
    "            the parallelism scheme for training. the default value is ParallelScheme.DEFAULT which means that tomotopy selects the best scheme by model.\"\"\",\n",
    "            \"Hyperparameters\": {\n",
    "                \"tw\": self.tw,\n",
    "                \"min_cf\": self.min_cf,\n",
    "                \"min_df\": self.min_df,\n",
    "                \"rm_top\": self.rm_top,\n",
    "                \"k\": self.k,\n",
    "                \"alpha\": self.alpha,\n",
    "                \"eta\": self.eta  ,\n",
    "                \"seed\": self.seed,\n",
    "                \"transform\": self.transform ,\n",
    "                \"iter\": self.iter\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(\"after Results initialization\")\n",
    "        \n",
    "        for k in range(self.k):\n",
    "            topic_words = model.get_topic_words(k, top_n=top_n)\n",
    "            results[\"Topic Words\"][\"topic{}\".format(k)] = topic_words\n",
    "\n",
    "        print(\"After Adding Topic Words\")\n",
    "\n",
    "        # # Adding visualizations to the results\n",
    "\n",
    "        results = self._visualize(results=results)\n",
    "\n",
    "        print(\"After adding Visualizations\")\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "class TextNormalizer:\n",
    "    def __init__(self, language=\"english\"):\n",
    "        \"\"\"\n",
    "        Initializes the TextNormalizer with a specified language for stopwords.\n",
    "\n",
    "        Args:\n",
    "            language (str): Language for stopwords. Default is 'english'.\n",
    "        \"\"\"\n",
    "        # Download stopwords if not already downloaded\n",
    "        try:\n",
    "            self.stop_words = set(stopwords.words(language))\n",
    "        except LookupError:\n",
    "            nltk.download(\"stopwords\")\n",
    "            self.stop_words = set(stopwords.words(language))\n",
    "\n",
    "        # Initialize the stemmer\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "        # Create a translation table for removing punctuation\n",
    "        self.punct_table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "        Normalizes the input text by lowercasing, removing punctuation,\n",
    "        removing stopwords, and applying stemming.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to normalize.\n",
    "\n",
    "        Returns:\n",
    "            str: The normalized text.\n",
    "        \"\"\"\n",
    "        # Lowercase the text\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = text.translate(self.punct_table)\n",
    "\n",
    "        # Remove numerical digits (optional)\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "        # Tokenize the text into words\n",
    "        tokens = text.split()\n",
    "\n",
    "        # Remove stopwords and apply stemming\n",
    "        processed_tokens = [\n",
    "            self.stemmer.stem(word)\n",
    "            for word in tokens\n",
    "            if word not in self.stop_words\n",
    "        ]\n",
    "\n",
    "        return processed_tokens\n",
    "\n",
    "def lda_topic_modeling(\n",
    "    excel_file_name,\n",
    "    column_name,\n",
    "    n_topics,\n",
    "    iterations=100,\n",
    "    language=\"english\",\n",
    "    n_top_words = 10\n",
    "):\n",
    "    text_normalizer = TextNormalizer(language=language)\n",
    "    lda_model = LatentDirichletAllocation(iter=iterations, k=n_topics)\n",
    "    \n",
    "    # Read the Excel file into a DataFrame\n",
    "    data_df = pd.read_excel(excel_file_name)\n",
    "    \n",
    "    # Normalize the text data\n",
    "    texts = data_df[column_name].astype(str).to_list()\n",
    "    normalized_texts = [text_normalizer(text) for text in texts]\n",
    "    documents = [{\"AbstractNormalized\": text} for text in normalized_texts]\n",
    "    \n",
    "    # Run LDA topic modeling\n",
    "    results = lda_model(documents, top_n=n_top_words)\n",
    "    \n",
    "    # Save top words for every topic into an Excel file\n",
    "    topic_words = results[\"Topic Words\"]\n",
    "    topic_word_list = []\n",
    "    for topic_key, words in topic_words.items():\n",
    "        topic_num = int(topic_key.replace('topic', ''))\n",
    "        for word, prob in words:\n",
    "            topic_word_list.append({'Topic': topic_num, 'Word': word, 'Probability': prob})\n",
    "    df_topic_words = pd.DataFrame(topic_word_list)\n",
    "    df_topic_words.to_excel('topic_top_words.xlsx', index=False)\n",
    "    \n",
    "    # Get topic distributions for each document\n",
    "    topic_distributions = [doc.get_topic_dist() for doc in lda_model.model.docs]\n",
    "    \n",
    "    # Create a DataFrame from topic distributions\n",
    "    n_topics = lda_model.k\n",
    "    topic_dist_df = pd.DataFrame(topic_distributions, columns=[f'Topic_{i}' for i in range(n_topics)])\n",
    "    \n",
    "    # Concatenate the topic distributions with the original data\n",
    "    data_with_topics = pd.concat([data_df.reset_index(drop=True), topic_dist_df], axis=1)\n",
    "    \n",
    "    # Save the combined DataFrame to an Excel file\n",
    "    data_with_topics.to_excel('document_topic_distributions.xlsx', index=False)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In LDA\n",
      "Normalized Documents\n",
      "Creating the model\n",
      "Adding the Documents\n",
      "Training the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lukas\\AppData\\Local\\Temp\\ipykernel_33000\\330123022.py:330: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n",
      "  model.train(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the results\n",
      "after Results initialization\n",
      "After Adding Topic Words\n",
      "After adding Visualizations\n"
     ]
    }
   ],
   "source": [
    "# Parameter\n",
    "\n",
    "excel_file_name = \"data\\\\Profi-Auszug_Aug24_erweitert_v2.xlsx\"\n",
    "\n",
    "# Welche Spalte aus der Excel soll verwendet werden\n",
    "column_name = \"Aufgabenbeschreibung\"\n",
    "\n",
    "# Anzahl an Topics die gebildet werden\n",
    "n_topics = 5 \n",
    "\n",
    "language = \"german\" # Sprache der Texte\n",
    "\n",
    "# Parameter für die Suche der optimalen anzahl an topics\n",
    "start = 2\n",
    "limit = 30\n",
    "step = 1\n",
    "\n",
    "# Wie viele Wörter pro topic Ausgegeben werden sollen\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "# Dauert Länger wenn hoch macht Ergebnis aber besser\n",
    "\n",
    "iterations = 2000\n",
    "\n",
    "lda_topic_modeling(excel_file_name=excel_file_name,\n",
    "                   column_name=column_name, \n",
    "                   n_topics=n_topics, \n",
    "                   iterations=iterations,\n",
    "                   language=language)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
